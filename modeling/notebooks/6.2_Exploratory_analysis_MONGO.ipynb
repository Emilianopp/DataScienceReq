{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo \n",
    "import pandas as pd \n",
    "import plotly as px \n",
    "import numpy as np \n",
    "import sys\n",
    "import pymongo\n",
    "from pymongo import MongoClient \n",
    "import re\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "sys.path.append(\"../DataPipe/\")\n",
    "from scraping.classes.DataBase.Mongo import *\n",
    "from scraping.classes.Role import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Analysis_Processing:\n",
    "    def __init__(self,db:Mongo,role:Role):\n",
    "        self.db = db\n",
    "        self.role = role\n",
    "    '''\n",
    "    if a pipeline is given, returns a dataframe with the pipeline query\n",
    "    if none is given, returns all model outputs as a dataframe \n",
    "    '''\n",
    "    def get_data(self,col,pipe=None) -> pd.DataFrame:\n",
    "        if pipe != None:\n",
    "            query_cursor = self.db.db[col].aggregate(pipe)\n",
    "        else:\n",
    "            query_cursor = self.db.db[col].find({},{\"_id\":0,\"text\":1})\n",
    "            query_cursor = {\"text\":[x.get(\"text\") for x in query_cursor]}\n",
    "        out = pd.DataFrame(query_cursor)\n",
    "        if ( out.empty):\n",
    "            raise Exception(\"DATA FRAME EMPTY\")\n",
    "        return out\n",
    "\n",
    "    '''\n",
    "    Strips symbols from sentence \n",
    "    still does not strip \",\" <- to fix\n",
    "    '''\n",
    "    def cleanse_sentence(self,sentence:str):\n",
    "        stop = stopwords.words('english')\n",
    "        sentence_clean = sentence.replace(\"-\", \" \")\n",
    "        sentence_clean = sentence.replace(\",\", \"\")\n",
    "        sentence_clean = re.sub(\"[\\n]\", \" \",sentence_clean)\n",
    "        sentence_clean = re.sub(\"[.!?/\\()-,:]\", \"\",sentence_clean)\n",
    "        sentence_clean = sentence_clean.lower()\n",
    "        sentence_clean = \" \".join([word for word in sentence_clean.split(\" \") if word not in stop])\n",
    "        return sentence_clean\n",
    "\n",
    "    '''\n",
    "    Strips digits from sentence\n",
    "    mostly used for bigram stuff\n",
    "    '''\n",
    "    def strip_digits_from_corpus(self,text):\n",
    "        subs = re.sub(\"[\\d+][+-]\", \"\",text)\n",
    "        subs = re.sub(\"[â€™']\", \"\",subs)\n",
    "        return(subs)\n",
    "\n",
    "    '''\n",
    "    finds keywords in model outputs\n",
    "    filters model outputs by url\n",
    "    this way we do not count repeats of words in a single posting\n",
    "    can do analysis with any txt file as long as the format is correct\n",
    "    '''\n",
    "    def do_analysis(self,keyword_path,col = None ):\n",
    "        urls = []\n",
    "        #load tech lits\n",
    "        tech_list = list(pd.read_csv(keyword_path ,index_col=[0]).iloc[:,0].str.lower())\n",
    "        #load all text \n",
    "        model_outs = self.db.db[col].find({},{\"text\":1, \"_id\":0,'urls':1 })\n",
    "        #iterate through text, for items in list\n",
    "        for text_dict in model_outs:\n",
    "            found = []\n",
    "            text = text_dict.get(\"text\")\n",
    "            text = self.cleanse_sentence(text)\n",
    "            for word in text.split(\" \"):\n",
    "                if word in tech_list:\n",
    "                   found.append(word)\n",
    "            urls.append({'url':text_dict.get(\"urls\"), 'tech_list': found})\n",
    "        return urls\n",
    "    '''\n",
    "    Bigram analysis still need to fix and touch up\n",
    "    '''\n",
    "    def bigram_analysis(self,df,thresh = 5,insert = False,col = \"bigrams\"):\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        corpus_list = [self.cleanse_sentence(self.strip_digits_from_corpus(sentence)) for sentence in df.text]\n",
    "        corpus = ' '.join(corpus_list)\n",
    "        finder = BigramCollocationFinder.from_words(corpus.lower().split(\" \"),window_size=2)\n",
    "        finder.apply_freq_filter(thresh)\n",
    "        bigram_results = finder.score_ngrams(bigram_measures.pmi)\n",
    "        if insert == True: \n",
    "            requests =  [InsertOne({\"bigram\":x[0],\"pmi\":x[1],'role':self.role.title}) for x in bigram_results]\n",
    "            self.db.db[col].write(requests)\n",
    "            print(\"successful insertion\")\n",
    "        else: \n",
    "            return bigram_results \n",
    "\n",
    "    def store_analysis(self,db,data:dict):\n",
    "        [{\"bigram\":x[0],\"pmi\":x[1]} for x in analysis.bigram_analysis(df = query_df)]\n",
    "        requests = [InsertOne(x) for x in data]\n",
    "        scrape_table.collection.bulk_write(requests)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = Mongo(client)\n",
    "analysis = Analysis_Processing(db,Role(\"Data Science\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert tech list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1ddf17f8f80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inserts = analysis.do_analysis(r\"C:\\Users\\Emiliano\\Documents\\Git\\DataScienceReq\\data\\languages.txt\" ,col = 'model_outputs')\n",
    "insert = [x for x in inserts if x.get(\"tech_list\") ]\n",
    "client.prod.techs.insert_many(insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1ddf1a0ac40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inserts = analysis.do_analysis(r\"C:\\Users\\Emiliano\\Documents\\Git\\DataScienceReq\\data\\DS_packages.txt\" ,col = 'model_outputs')\n",
    "insert = [x for x in inserts if x.get(\"tech_list\") ]\n",
    "client.prod.packages.insert_many(insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimental bigram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('computer', 'science'), 5.904216461310195),\n",
       " (('machine', 'learning'), 5.2593287072523545),\n",
       " (('deep', 'learning'), 4.854781432686481),\n",
       " (('', 'years'), 4.654327793286782),\n",
       " (('', ''), 4.056816829880567),\n",
       " (('experience', 'working'), 3.8480643710755658),\n",
       " (('years', 'experience'), 3.463400520840244)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = analysis.get_data(col = \"model_outputs\")\n",
    "analysis.bigram_analysis(df,thresh= 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [{\n",
    "   '$lookup':\n",
    "     {\n",
    "       'from': \"Scraped_Data\",\n",
    "      'localField': \"urls\",\n",
    "      'foreignField':\"url\",\n",
    "      'as': \"test\"\n",
    "     }\n",
    "},\n",
    "{'$match': {\"test.country\": \"Canada\" }}\n",
    "]\n",
    "\n",
    "query_df = client.prod['packages'].aggregate(pipe)\n",
    "list(query_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ffc7893c692848b0397a1b2e1036e6e58d5f9824a51a1f3b5e4a482673fe79a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('DataSci': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

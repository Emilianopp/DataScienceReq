{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\n",
    "#log into linked in\n",
    "def login(driver,p):\n",
    "  url =  \"https://www.linkedin.com/checkpoint/rm/sign-in-another-account?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\"\n",
    "  wait = WebDriverWait(driver, 10)\n",
    "  driver.get(url)\n",
    "  \n",
    "  username = driver.find_element_by_id(\"username\")\n",
    "  username.send_keys(\"emilianopp550@gmail.com\")\n",
    "  password = driver.find_element_by_id(\"password\")\n",
    "  password.send_keys(p)\n",
    "  driver.find_element_by_class_name(\"login__form_action_container\").click()\n",
    "\n",
    "#redirects driver to the query link\n",
    "def search(driver):\n",
    "  time.sleep(5)\n",
    "  driver.get(\"https://www.linkedin.com/jobs/search/?keywords=data%20science\")\n",
    "\n",
    "#grabs results fetched\n",
    "def get_n_results(driver):\n",
    "  time.sleep(10)\n",
    "  results_div = driver.find_element_by_xpath(\"/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/header/div[1]/small\")\n",
    "  n_string = results_div.text\n",
    "  n = int(n_string.split()[0].replace(',',\"\"))\n",
    "  return n \n",
    "\n",
    "#Finds job ul div\n",
    "def get_jobs(driver):\n",
    "  ul_div = driver.find_element_by_xpath(\"/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul\")\n",
    "  return ul_div\n",
    "#Scrolls to properly load page\n",
    "def scroll_down(driver):\n",
    "  for i in np.linspace(0,1,10):\n",
    "    time.sleep(2)\n",
    "    driver.execute_script(f\"window.scrollTo(0,document.body.scrollHeight*{i})\")\n",
    "def get_job_urls(jobs,driver,job_urls = {}):\n",
    "  i = 1\n",
    "  #Collects job urls,location role cand company \n",
    "  #the final result updates the input dictionary and appends a key value pair with the format\n",
    "  #    url:{'company':company,'location':location,'role':role}\n",
    "  while True: \n",
    "    try:\n",
    "      WebDriverWait(driver, 1).until(EC.presence_of_element_located((By.XPATH,f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]')))\n",
    "      url = jobs.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[1]/a').get_attribute(\"href\")\n",
    "      role = jobs.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[1]/a').text\n",
    "      company = jobs.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[1]/a').text\n",
    "      location = driver.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[3]/ul/li').text\n",
    "      job_urls.update({url:{'company':company,'location':location,'role':role}})\n",
    "      i+=1\n",
    "    except:\n",
    "      return job_urls\n",
    "\n",
    "def load_next_page(driver):\n",
    "  #loads next page for url retrival\n",
    "  curr= driver.find_element_by_xpath('//*[@aria-current=\"true\"]').text\n",
    "  next = driver.find_element_by_xpath(f'//*[@aria-label=\"Page {int(curr)+1}\"]')\n",
    "  next.click()\n",
    "\n",
    "def get_description(driver,job_dict,good):\n",
    "\n",
    "  fail = []\n",
    "  #Iterate through the url list to scrape the descriptions\n",
    "  for url in list(job_dict.keys()):\n",
    "    if url not in good:\n",
    "      try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        if driver.current_url != url:\n",
    "          print(f'failed at {url}')\n",
    "          #remove borken urls\n",
    "          job_dict.pop(url)\n",
    "        #scrape\n",
    "        driver.find_element_by_xpath('//*[@aria-label=\"Click to see more description\"]').click()\n",
    "        description = driver.find_element_by_xpath('/html/body/div[6]/div[3]/div/div[1]/div[1]/div/div[2]/article/div').text\n",
    "        job_dict.get(url).update({\"description\":description})\n",
    "        good.append(url)\n",
    "        return job_dict\n",
    "      except:\n",
    "        #keep going if there is a random error in which a div did not load properly but check where we failed\n",
    "        print(f\"fail {job_dict.get(url)}\")\n",
    "        fail.append(url)\n",
    "  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#Run theough the entire process of fetch in urls logining in and grabing job descriptions\n",
    "def main(driver,password):\n",
    "    login(driver,password)\n",
    "    search(driver)\n",
    "    n = get_n_results(driver)\n",
    "    job_dict ={}\n",
    "    #iterate through the amount of pages given\n",
    "    for i in range(40):\n",
    "        jobs = get_jobs(driver)\n",
    "        scroll_down(driver)\n",
    "        get_job_urls(jobs,driver,job_urls = job_dict)\n",
    "        load_next_page(driver)\n",
    "    get_description(driver,job_dict,good)\n",
    "\n",
    "    return get_description(driver,job_dict,good)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "p = open(\"../../pass.txt\", \"r\").read()\n",
    "type(f)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "driver = webdriver.Chrome(r'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe')\n",
    "f = open(\"../../pass.txt\", \"r\").read()\n",
    "main(driver,f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "#use pickle object storage to store dictionary items for later use\n",
    "\n",
    "def save(final_file):\n",
    "    with open(f'../data/{final_file}.p', 'wb') as fp:\n",
    "        pickle.dump(final_file,file=fp,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "def load():\n",
    "    with open('../data/job_dict.p', 'rb') as fp:\n",
    "        job_dict = pickle.load(fp)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DataSci': conda)"
  },
  "interpreter": {
   "hash": "1ffc7893c692848b0397a1b2e1036e6e58d5f9824a51a1f3b5e4a482673fe79a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data formatting and Preprocessing "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Methadology: \r\n",
    "\r\n",
    "1. Gather data science/data related jobs\r\n",
    "    * General Business/Actuarial analyst positions grabbed that are not of interest\r\n",
    "2.  Split descriptions at \\n tags to gather sentences used for classification converts string gathered to list\r\n",
    "3. Cleanse list by removing empty indexes \r\n",
    "4. Convert to  np array and lemmatize data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.common.by import By\r\n",
    "from selenium.webdriver.common.keys import Keys\r\n",
    "from selenium.webdriver.support.ui import WebDriverWait\r\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\r\n",
    "from selenium.webdriver.support import expected_conditions as EC\r\n",
    "import pandas as pd\r\n",
    "import nltk\r\n",
    "from nltk import WordNetLemmatizer \r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "import joblib\r\n",
    "import pickle\r\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "# load saved data \r\n",
    "def load():\r\n",
    "    with open('../data/job_dict.p', 'rb') as fp:\r\n",
    "        job_dict = pickle.load(fp)\r\n",
    "    return job_dict\r\n",
    "\r\n",
    "def save(final_file):\r\n",
    "    with open(f'../data/job_dict.p', 'wb') as fp:\r\n",
    "        pickle.dump(final_file,file=fp,protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "job_dict = load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "data_science = []\r\n",
    "machine_learning = []\r\n",
    "analyst = []\r\n",
    "art_int = []\r\n",
    "left_over= []\r\n",
    "statistics= []\r\n",
    "for listing,key in zip(job_dict,list(job_dict.keys())):\r\n",
    "    inf = job_dict.get(key)\r\n",
    "    role = inf.get('role')\r\n",
    "    ds = bool(re.search(\".*[Dd]ata\\s.[Sc]ien.*\", role))\r\n",
    "    ml = bool(re.search(\".*[Mm]achine\\s[Ll]earn.*\",role))\r\n",
    "    anal = bool(re.search(\".*[Dd]ata\\s[Aa]nal.*\", role))\r\n",
    "    ai_long = bool(re.search(\".*\\s[Aa]rtificial\\s[Ii]ntelligence\\s.*\", role))\r\n",
    "    ai = bool(re.search(\".*\\s[Aa][Ii]\\s.*\", role))\r\n",
    "    stat = bool(re.search(\".*[St]at.*\\s[Pp]rogram.*\", role))\r\n",
    "    if(ds):\r\n",
    "        data_science.append(listing)\r\n",
    "    elif(ml):\r\n",
    "        machine_learning.append(listing)\r\n",
    "    elif(anal):\r\n",
    "        analyst.append(listing)\r\n",
    "    elif((ai or ai_long) and listing not in data_science and listing not in analyst and listing not in machine_learning):\r\n",
    "        art_int.append(listing)\r\n",
    "    elif(stat and listing not in data_science and listing not in analyst and listing not in machine_learning):\r\n",
    "        statistics.append(listing)\r\n",
    "    else:\r\n",
    "        left_over.append(listing)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "print(len(analyst),len(machine_learning),len(data_science),len(statistics),len(art_int))\r\n",
    "data = (analyst)+(machine_learning)+(data_science)+(statistics)+(art_int)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "84 83 201 30 6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "def role_get(data):\r\n",
    "    role = []\r\n",
    "    for i in data:\r\n",
    "        listing= job_dict.get(i)\r\n",
    "        role.append(listing.get('role'))\r\n",
    "    return role\r\n",
    "roles = role_get(data) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "#gets description of data \r\n",
    "def description_getter(data):\r\n",
    "    descriptions=[]\r\n",
    "    for data_point in data:\r\n",
    "        try:\r\n",
    "            desc = job_dict.get(data_point).get('description').split(\"\\n\")\r\n",
    "            descriptions.append(desc)\r\n",
    "        except AttributeError:\r\n",
    "        #remove depreciated urls from dataset\r\n",
    "            print(f'broken urls {data_point}')\r\n",
    "            job_dict.pop(data_point)\r\n",
    "            save(job_dict)\r\n",
    "    return descriptions\r\n",
    "segregated_data = description_getter(data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "#Extracts empty string list objects from data\r\n",
    "def cleanse_empty_data(data):\r\n",
    "   prod_data = []\r\n",
    "   for k in data:\r\n",
    "      [prod_data.append(x) for x in k if x !=\"\"]\r\n",
    "   return prod_data\r\n",
    "\r\n",
    "prod_data = cleanse_empty_data(segregated_data)\r\n",
    "arr_prod_data = np.array(prod_data)\r\n",
    "lem = WordNetLemmatizer()\r\n",
    "Lemmatizer = np.vectorize(lambda x: ' '.join([lem.lemmatize(i) for i in x.split(\" \")]))\r\n",
    "lem_data = Lemmatizer(prod_data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "pd.Series(lem_data).to_csv(\"../data/processed_data.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DataSci': conda)"
  },
  "interpreter": {
   "hash": "1ffc7893c692848b0397a1b2e1036e6e58d5f9824a51a1f3b5e4a482673fe79a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "from selenium import webdriver\r\n",
    "from selenium.webdriver.common.by import By\r\n",
    "from selenium.webdriver.common.keys import Keys\r\n",
    "from selenium.webdriver.support.ui import WebDriverWait\r\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\r\n",
    "from selenium.webdriver.support import expected_conditions as EC\r\n",
    "import time\r\n",
    "import numpy as np\r\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "\r\n",
    "#log into linked in\r\n",
    "def login(driver):\r\n",
    "  url =  \"https://www.linkedin.com/checkpoint/rm/sign-in-another-account?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\"\r\n",
    "  wait = WebDriverWait(driver, 10)\r\n",
    "  driver.get(url)\r\n",
    "  \r\n",
    "  username = driver.find_element_by_id(\"username\")\r\n",
    "  username.send_keys(\"emilianopp550@gmail.com\")\r\n",
    "  password = driver.find_element_by_id(\"password\")\r\n",
    "  password.send_keys(\"\")\r\n",
    "  driver.find_element_by_class_name(\"login__form_action_container\").click()\r\n",
    "\r\n",
    "#redirects driver to the query link\r\n",
    "def search(driver):\r\n",
    "  time.sleep(5)\r\n",
    "  driver.get(\"https://www.linkedin.com/jobs/search/?keywords=data%20science\")\r\n",
    "\r\n",
    "#grabs results fetched\r\n",
    "def get_n_results(driver):\r\n",
    "  time.sleep(10)\r\n",
    "  results_div = driver.find_element_by_xpath(\"/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/header/div[1]/small\")\r\n",
    "  n_string = results_div.text\r\n",
    "  n = int(n_string.split()[0].replace(',',\"\"))\r\n",
    "  return n \r\n",
    "\r\n",
    "#Finds job ul div\r\n",
    "def get_jobs(driver):\r\n",
    "  ul_div = driver.find_element_by_xpath(\"/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul\")\r\n",
    "  return ul_div\r\n",
    "#Scrolls to properly load page\r\n",
    "def scroll_down(driver):\r\n",
    "  for i in np.linspace(0,1,10):\r\n",
    "    time.sleep(2)\r\n",
    "    driver.execute_script(f\"window.scrollTo(0,document.body.scrollHeight*{i})\")\r\n",
    "def get_job_urls(jobs,driver,job_urls = {}):\r\n",
    "  i = 1\r\n",
    "  #Collects job urls,location role cand company \r\n",
    "  #the final result updates the input dictionary and appends a key value pair with the format\r\n",
    "  #    url:{'company':company,'location':location,'role':role}\r\n",
    "  while True: \r\n",
    "    try:\r\n",
    "      WebDriverWait(driver, 1).until(EC.presence_of_element_located((By.XPATH,f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]')))\r\n",
    "      url = jobs.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[1]/a').get_attribute(\"href\")\r\n",
    "      role = jobs.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[1]/a').text\r\n",
    "      company = jobs.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[1]/a').text\r\n",
    "      location = driver.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{i}]/div/div/div[1]/div[2]/div[3]/ul/li').text\r\n",
    "      job_urls.update({url:{'company':company,'location':location,'role':role}})\r\n",
    "      i+=1\r\n",
    "    except:\r\n",
    "      return job_urls\r\n",
    "\r\n",
    "def load_next_page(driver):\r\n",
    "  #loads next page for url retrival\r\n",
    "  curr= driver.find_element_by_xpath('//*[@aria-current=\"true\"]').text\r\n",
    "  next = driver.find_element_by_xpath(f'//*[@aria-label=\"Page {int(curr)+1}\"]')\r\n",
    "  next.click()\r\n",
    "\r\n",
    "def get_description(driver,job_dict,good):\r\n",
    "\r\n",
    "  fail = []\r\n",
    "  #Iterate through the url list to scrape the descriptions\r\n",
    "  for url in list(job_dict.keys()):\r\n",
    "    if url not in good:\r\n",
    "      try:\r\n",
    "        driver.get(url)\r\n",
    "        time.sleep(3)\r\n",
    "        if driver.current_url != url:\r\n",
    "          print(f'failed at {url}')\r\n",
    "          #remove borken urls\r\n",
    "          job_dict.pop(url)\r\n",
    "        #scrape\r\n",
    "        driver.find_element_by_xpath('//*[@aria-label=\"Click to see more description\"]').click()\r\n",
    "        description = driver.find_element_by_xpath('/html/body/div[6]/div[3]/div/div[1]/div[1]/div/div[2]/article/div').text\r\n",
    "        job_dict.get(url).update({\"description\":description})\r\n",
    "        good.append(url)\r\n",
    "        \r\n",
    "      except:\r\n",
    "        #keep going if there is a random error in which a div did not load properly but check where we failed\r\n",
    "        print(f\"fail {job_dict.get(url)}\")\r\n",
    "        fail.append(url)\r\n",
    "  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "#Run theough the entire process of fetchin urls logining in and grabing job descriptions\r\n",
    "def main(driver):\r\n",
    "\r\n",
    "    login(driver)\r\n",
    "    search(driver)\r\n",
    "    n = get_n_results(driver)\r\n",
    "    job_dict ={}\r\n",
    "    #iterate through the amount of pages given\r\n",
    "    for i in range(40):\r\n",
    "        jobs = get_jobs(driver)\r\n",
    "        scroll_down(driver)\r\n",
    "        get_job_urls(jobs,driver,job_urls = job_dict)\r\n",
    "        load_next_page(driver)\r\n",
    "    get_description(driver,job_dict,good)\r\n",
    "\r\n",
    "    return job_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "#use pickle object storage to store dictionary items for later use\r\n",
    "\r\n",
    "def save():\r\n",
    "    with open('C:/Users/Emiliano/Documents/Git/DataScienceReq/job_dict.p', 'wb') as fp:\r\n",
    "        pickle.dump(job_dict,file=fp,protocol=pickle.HIGHEST_PROTOCOL)\r\n",
    "def load():\r\n",
    "    with open('job_dict.p', 'rb') as fp:\r\n",
    "        job_dict = pickle.load(fp)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "961d25575582502f5deaa39b930dbe1b811639c5b9aea0b446aa7add69b24b8e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}